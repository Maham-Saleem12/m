{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ann proj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPdFdTyvyhcWCZV0bY/+U1P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maham-Saleem12/m/blob/main/Ann_proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "m74rNPkGoe7s",
        "outputId": "a6c85277-678c-474c-845e-1445eec4b676"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-de6a29d1-c8ef-4bb3-ade5-8daacba335e9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-de6a29d1-c8ef-4bb3-ade5-8daacba335e9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving audit_data.csv to audit_data (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3pV2-THoFLC"
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv('audit_data.csv')"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "X88hhOQCosq0",
        "outputId": "b8af3f3e-d735-4b85-b942-a40e119cf287"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sector_score</th>\n",
              "      <th>LOCATION_ID</th>\n",
              "      <th>PARA_A</th>\n",
              "      <th>Score_A</th>\n",
              "      <th>Risk_A</th>\n",
              "      <th>PARA_B</th>\n",
              "      <th>Score_B</th>\n",
              "      <th>Risk_B</th>\n",
              "      <th>TOTAL</th>\n",
              "      <th>numbers</th>\n",
              "      <th>Score_B.1</th>\n",
              "      <th>Risk_C</th>\n",
              "      <th>Money_Value</th>\n",
              "      <th>Score_MV</th>\n",
              "      <th>Risk_D</th>\n",
              "      <th>District_Loss</th>\n",
              "      <th>PROB</th>\n",
              "      <th>RiSk_E</th>\n",
              "      <th>History</th>\n",
              "      <th>Prob</th>\n",
              "      <th>Risk_F</th>\n",
              "      <th>Score</th>\n",
              "      <th>Inherent_Risk</th>\n",
              "      <th>CONTROL_RISK</th>\n",
              "      <th>Detection_Risk</th>\n",
              "      <th>Audit_Risk</th>\n",
              "      <th>Risk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.89</td>\n",
              "      <td>23</td>\n",
              "      <td>4.18</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2.508</td>\n",
              "      <td>2.50</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.500</td>\n",
              "      <td>6.68</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.38</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.676</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>8.574</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.7148</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.89</td>\n",
              "      <td>6</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4.83</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.966</td>\n",
              "      <td>4.83</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.188</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.554</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5108</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.89</td>\n",
              "      <td>6</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.74</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.548</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.3096</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.89</td>\n",
              "      <td>6</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>10.80</td>\n",
              "      <td>0.6</td>\n",
              "      <td>6.480</td>\n",
              "      <td>10.80</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>3.6</td>\n",
              "      <td>11.75</td>\n",
              "      <td>0.6</td>\n",
              "      <td>7.050</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.4</td>\n",
              "      <td>17.530</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>3.5060</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.89</td>\n",
              "      <td>6</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.08</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.416</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.2832</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>771</th>\n",
              "      <td>55.57</td>\n",
              "      <td>9</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.89</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.578</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.3156</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>772</th>\n",
              "      <td>55.57</td>\n",
              "      <td>16</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.84</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.568</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.3136</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>773</th>\n",
              "      <td>55.57</td>\n",
              "      <td>14</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.28</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.456</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.2912</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>774</th>\n",
              "      <td>55.57</td>\n",
              "      <td>18</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.20</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.440</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.2880</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>775</th>\n",
              "      <td>55.57</td>\n",
              "      <td>15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.064</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.464</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.2928</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>776 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Sector_score  LOCATION_ID  PARA_A  ...  Detection_Risk  Audit_Risk  Risk\n",
              "0            3.89           23    4.18  ...             0.5      1.7148     1\n",
              "1            3.89            6    0.00  ...             0.5      0.5108     0\n",
              "2            3.89            6    0.51  ...             0.5      0.3096     0\n",
              "3            3.89            6    0.00  ...             0.5      3.5060     1\n",
              "4            3.89            6    0.00  ...             0.5      0.2832     0\n",
              "..            ...          ...     ...  ...             ...         ...   ...\n",
              "771         55.57            9    0.49  ...             0.5      0.3156     0\n",
              "772         55.57           16    0.47  ...             0.5      0.3136     0\n",
              "773         55.57           14    0.24  ...             0.5      0.2912     0\n",
              "774         55.57           18    0.20  ...             0.5      0.2880     0\n",
              "775         55.57           15    0.00  ...             0.5      0.2928     0\n",
              "\n",
              "[776 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfXHiJfdo6_z"
      },
      "source": [
        "df = dataset.values #convert our dataframe into array"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsPqrHDypFTL",
        "outputId": "3ef214f5-0af2-4ff8-95e4-8d213b2f7e55"
      },
      "source": [
        "\n",
        "df"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.89  , 23.    ,  4.18  , ...,  0.5   ,  1.7148,  1.    ],\n",
              "       [ 3.89  ,  6.    ,  0.    , ...,  0.5   ,  0.5108,  0.    ],\n",
              "       [ 3.89  ,  6.    ,  0.51  , ...,  0.5   ,  0.3096,  0.    ],\n",
              "       ...,\n",
              "       [55.57  , 14.    ,  0.24  , ...,  0.5   ,  0.2912,  0.    ],\n",
              "       [55.57  , 18.    ,  0.2   , ...,  0.5   ,  0.288 ,  0.    ],\n",
              "       [55.57  , 15.    ,  0.    , ...,  0.5   ,  0.2928,  0.    ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnroA-oYppp4"
      },
      "source": [
        "#split our dataset into X input features and y(predict/label)\n",
        "X = df[:,0:26]\n",
        "Y = df[:,26]"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8QM7OwFrnCS"
      },
      "source": [
        "#preprocess our dataset/scale/normalize our ds\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNqHgf8FrvcZ",
        "outputId": "5bf353ec-24c4-445c-88b7-a7948e20b481"
      },
      "source": [
        "X"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.67046481,  0.82543008,  0.30480033, ..., -0.38866169,\n",
              "         0.        , -0.14112307],\n",
              "       [-0.67046481, -0.8953464 , -0.43173627, ..., -0.38866169,\n",
              "         0.        , -0.17228042],\n",
              "       [-0.67046481, -0.8953464 , -0.34187176, ..., -0.38866169,\n",
              "         0.        , -0.17748711],\n",
              "       ...,\n",
              "       [ 1.45599165, -0.08556924, -0.38944709, ..., -0.38866169,\n",
              "         0.        , -0.17796327],\n",
              "       [ 1.45599165,  0.31931935, -0.39649529, ..., -0.38866169,\n",
              "         0.        , -0.17804608],\n",
              "       [ 1.45599165,  0.01565291, -0.43173627, ..., -0.38866169,\n",
              "         0.        , -0.17792186]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9avDoNvTsOKq"
      },
      "source": [
        "#split ds into test,train\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X, Y, test_size=0.3)\n",
        "#X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.3,random_state = 0)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbTSrzy2teBG"
      },
      "source": [
        "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3AMVIuvth-x",
        "outputId": "9f6ed2a8-7057-4e97-ae1d-9d40d5dc16ea"
      },
      "source": [
        "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(543, 26) (116, 26) (117, 26) (543,) (116,) (117,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIB9JMT_tu8g",
        "outputId": "fe794ef0-b83b-4f02-947c-65fd16a7db62"
      },
      "source": [
        "#build ann now\n",
        "#first import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#keras sequential model///store model in variable and describe layer by layer \n",
        "model = Sequential([\n",
        "    Dense(32, activation='relu', input_shape=(26,)),   #first layer with 32 neurons and 26 input features(columns)\n",
        "    Dense(32, activation='relu'),     #second layer\n",
        "    Dense(2, activation='sigmoid'),  #third layer with 2 neuron and sigmoid active.\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_63 (Dense)             (None, 32)                864       \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 1,986\n",
            "Trainable params: 1,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz27kq0WvUag"
      },
      "source": [
        "#configure the model, which optimizer algo,loss function, and metrics we use\n",
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuNz936Kzf4Q",
        "outputId": "d0920919-d94c-4247-c5ff-7938cc5c36e5"
      },
      "source": [
        "hist = model.fit(X_train, Y_train,\n",
        "          batch_size=32, epochs=100,\n",
        "          validation_data=(X_val, Y_val))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "17/17 [==============================] - 1s 32ms/step - loss: nan - accuracy: 0.5628 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 2/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6212 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 3/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6462 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 4/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6106 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 5/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6578 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 6/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6519 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 7/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6377 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 8/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6295 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 9/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6662 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 10/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6214 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 11/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6673 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 12/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6490 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 13/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6254 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 14/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6594 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 15/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6442 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 16/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6235 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 17/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6255 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 18/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6622 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 19/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6376 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 20/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6550 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 21/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6533 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 22/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6260 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 23/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6265 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 24/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6670 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 25/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6438 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 26/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6510 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 27/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6191 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 28/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6526 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 29/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6344 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 30/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6038 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 31/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6342 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 32/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5971 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 33/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6126 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 34/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6425 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 35/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6177 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 36/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6366 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 37/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6379 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 38/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6279 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 39/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6339 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 40/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6317 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 41/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6469 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 42/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6314 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 43/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6278 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 44/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6409 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 45/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6329 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 46/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6488 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 47/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6303 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 48/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6174 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 49/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6356 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 50/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6374 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 51/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6167 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 52/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6322 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 53/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6285 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 54/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6324 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 55/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6422 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 56/100\n",
            "17/17 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.6242 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 57/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6736 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 58/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6405 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 59/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6562 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 60/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6066 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 61/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6376 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 62/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6391 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 63/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6579 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 64/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6281 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 65/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6193 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 66/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6280 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 67/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6254 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 68/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6486 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 69/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6361 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 70/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6194 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 71/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6315 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 72/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6270 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 73/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6253 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 74/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6234 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 75/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6433 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 76/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6550 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 77/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6435 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 78/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6247 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 79/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6236 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 80/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6568 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 81/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6263 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 82/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6227 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 83/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6347 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 84/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6430 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 85/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6363 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 86/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6221 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 87/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6157 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 88/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6289 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 89/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6474 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 90/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6422 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 91/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6698 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 92/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6316 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 93/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6217 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 94/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6488 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 95/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6220 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 96/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6063 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 97/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6199 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 98/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6243 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 99/100\n",
            "17/17 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.6473 - val_loss: nan - val_accuracy: 0.5259\n",
            "Epoch 100/100\n",
            "17/17 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6267 - val_loss: nan - val_accuracy: 0.5259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI8AsD6MXwnY",
        "outputId": "1462d6c2-7b44-4012-d9c4-ecff14a597a6"
      },
      "source": [
        "loss,accuracy = model.evaluate(X_test, Y_test)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.5641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GdFS-buuzOf",
        "outputId": "4d688101-c2d5-4887-8075-0aea5b32acda"
      },
      "source": [
        "\n",
        "print('Loss: %.2f' % loss)\n",
        "print('Accuracy is:', accuracy*100)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: nan\n",
            "Accuracy is: 56.41025900840759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTTw9noL4GXj",
        "outputId": "2603cc72-e62f-4a4a-d61f-5648bed38ac0"
      },
      "source": [
        "predic=model.predict(X)\n",
        "print(X)\n",
        "print(\"X=%s, Predicted=%s\" % (X[0], Y[0]))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.67046481  0.82543008  0.30480033 ... -0.38866169  0.\n",
            "  -0.14112307]\n",
            " [-0.67046481 -0.8953464  -0.43173627 ... -0.38866169  0.\n",
            "  -0.17228042]\n",
            " [-0.67046481 -0.8953464  -0.34187176 ... -0.38866169  0.\n",
            "  -0.17748711]\n",
            " ...\n",
            " [ 1.45599165 -0.08556924 -0.38944709 ... -0.38866169  0.\n",
            "  -0.17796327]\n",
            " [ 1.45599165  0.31931935 -0.39649529 ... -0.38866169  0.\n",
            "  -0.17804608]\n",
            " [ 1.45599165  0.01565291 -0.43173627 ... -0.38866169  0.\n",
            "  -0.17792186]]\n",
            "X=[-0.67046481  0.82543008  0.30480033  1.42984618  0.33650215 -0.16582948\n",
            " -0.6667522  -0.194121   -0.12750609 -0.25599783 -0.29528463 -0.28481157\n",
            " -0.16161448 -0.56989549 -0.18999669 -0.41140172 -0.1650196  -0.41041721\n",
            " -0.19669061 -0.24656792 -0.17539763 -0.35250258 -0.16646776 -0.38866169\n",
            "  0.         -0.14112307], Predicted=1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "MmB0NnfeYSAj",
        "outputId": "56f0cb81-5b51-4e98-d724-d2c18499ddb2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAc70lEQVR4nO3df5xVdb3v8dfbGYZBEZAfmjIklPiz/FFzOJWn8kd2KUvqaArVVSqzLNMy82i3H2rHe08dK7N8eK+/UvshGpoHvSoZab8tBiUSyEBCGUUcUGdMGJmBz/ljrYE12zWyJ2axh73fz8djP9jru9ba+7NcPvZ7vt/v3mspIjAzMyu1S6ULMDOzwckBYWZmuRwQZmaWywFhZma5HBBmZpbLAWFmZrkcEFbzJE2UFJLqy9h2pqTf7Ii6zCrNAWE7FUkrJW2UNLak/eH0Q35iZSozqz4OCNsZ/Q2Y0bMg6fXArpUrZ3Aopwdk1h8OCNsZ/QA4NbN8GnBTdgNJIyXdJKlN0uOSviRpl3RdnaTLJK2VtAI4Pmff6yStlvSkpH+XVFdOYZJ+IulpSe2SfiXpkMy6YZK+mdbTLuk3koal6/5F0u8kPS9plaSZafsDkk7PvEavIa601/RpScuAZWnbd9LX6JC0QNJbM9vXSfqipMckvZCunyDpSknfLDmWOZI+V85xW3VyQNjO6EFghKSD0g/u6cAPS7b5LjASeA3wdpJA+Ui67uPAe4AjgGbgpJJ9bwC6gf3Sbd4JnE557gEmA3sCDwE/yqy7DHgj8BZgNHA+sFnSvul+3wXGAYcDC8t8P4D3Af8MHJwuz09fYzTwY+AnkhrTdeeS9L7eDYwAPgqsB24EZmRCdCzwjnR/q1UR4YcfO80DWEnywfUl4P8AU4H7gHoggIlAHbARODiz3yeAB9LnvwA+mVn3znTfemAv4CVgWGb9DOD+9PlM4Ddl1joqfd2RJH+MbQAOy9nuQuCnfbzGA8DpmeVe75++/jHbqOO5nvcFHgWm9bHdUuC49PlZwN2VPt9+VPbhMUvbWf0A+BUwiZLhJWAsMAR4PNP2ODA+fb4PsKpkXY99031XS+pp26Vk+1xpb+ZS4AMkPYHNmXqGAo3AYzm7TuijvVy9apN0HvAxkuMMkp5Cz6T+K73XjcCHSQL3w8B3tqMmqwIeYrKdUkQ8TjJZ/W7g9pLVa4Eukg/7Hq8Gnkyfryb5oMyu67GKpAcxNiJGpY8REXEI2/ZBYBpJD2ckSW8GQGlNncBrc/Zb1Uc7wIv0noB/Vc42Wy7JnM43nA+cDOwREaOA9rSGbb3XD4Fpkg4DDgLu6GM7qxEOCNuZfYxkeOXFbGNEbAJuBS6VtHs6xn8uW+cpbgXOltQkaQ/ggsy+q4GfAd+UNELSLpJeK+ntZdSzO0m4rCP5UP/fmdfdDFwPfEvSPulk8ZslDSWZp3iHpJMl1UsaI+nwdNeFwL9K2lXSfukxb6uGbqANqJf0FZIeRI9rga9JmqzEoZLGpDW2ksxf/AC4LSI2lHHMVsUcELbTiojHIqKlj9WfIfnrewXwG5LJ1uvTddcAc4E/kUwkl/ZATgUagCUk4/ezgb3LKOkmkuGqJ9N9HyxZfx7wZ5IP4WeBrwO7RMQTJD2hz6ftC4HD0n2+TTKfsoZkCOhHvLK5wL3AX9NaOuk9BPUtkoD8GdABXAcMy6y/EXg9SUhYjVOEbxhkZglJbyPpae0b/nCoee5BmBkAkoYA5wDXOhwMHBBmBkg6CHieZCjt8gqXY4OEh5jMzCyXexBmZparan4oN3bs2Jg4cWKlyzAz26ksWLBgbUSMy1tXNQExceJEWlr6+sajmZnlkfR4X+s8xGRmZrkcEGZmlssBYWZmuRwQZmaWywFhZma5HBBmZpbLAWFmZrmq5ncQA+X2h1pZufbFbW9oZjZIvGrkMD74z6/e9ob95IDI6OzaxLm3/gmArXebNDMb3A6fMMoBUbSOzi4Avva+1/E/37TvNrY2M6tunoPI6NiQBMSIRuemmZkDIqM9DYiRw4ZUuBIzs8pzQGR0bOgGHBBmZuCA6KWnBzHCAWFm5oDI8hCTmdlWDoiMrZPUDggzMwdERvuGLoYNqaOh3v9ZzMz8SZjR0dnl4SUzs5QDIqN9Qxcjhvk3EGZm4IDopX2DexBmZj0cEBkdG7odEGZmKQdERvuGLn+Dycws5YDI6Ojs8o/kzMxSDojUps3BC53dDggzs5QDIvVCp39FbWaW5YBI+UJ9Zma9OSBS7b4XhJlZLw6IlC/UZ2bWmwMi1XO70ZG7OiDMzMABsUW7r+RqZtZLoQEhaaqkRyUtl3RBH9ucLGmJpMWSfpy2HS7p92nbIkmnFFknbL3Ut4eYzMwShc3ISqoDrgSOA1qB+ZLmRMSSzDaTgQuBIyPiOUl7pqvWA6dGxDJJ+wALJM2NiOeLqrd9Qxd1u4hdG+qKegszs51KkT2IKcDyiFgRERuBWcC0km0+DlwZEc8BRMQz6b9/jYhl6fOngGeAcQXWuuVCfZKKfBszs51GkQExHliVWW5N27L2B/aX9FtJD0qaWvoikqYADcBjOevOkNQiqaWtrW27iu3o9IX6zMyyKj1JXQ9MBo4CZgDXSBrVs1LS3sAPgI9ExObSnSPi6ohojojmceO2r4ORXKjPv4EwM+tRZEA8CUzILDelbVmtwJyI6IqIvwF/JQkMJI0A/j/wvyLiwQLrBJJJal+HycxsqyIDYj4wWdIkSQ3AdGBOyTZ3kPQekDSWZMhpRbr9T4GbImJ2gTVu0eGbBZmZ9VJYQEREN3AWMBdYCtwaEYslXSLphHSzucA6SUuA+4EvRMQ64GTgbcBMSQvTx+FF1Qo9txt1QJiZ9Sh00D0i7gbuLmn7SuZ5AOemj+w2PwR+WGRtJe9HR6d7EGZmWZWepB4UNnRtomtT+FfUZmYZDgh8oT4zszwOCHwvCDOzPA4IMhfqG+bfQZiZ9XBA4Av1mZnlcUDgOQgzszwOCHwvCDOzPA4Itt5Nbndfi8nMbAsHBEkPYvjQeurr/J/DzKyHPxFJvubq+Qczs94cEPg6TGZmeRwQpJf69vyDmVkvDgjwhfrMzHI4INh6P2ozM9vKAYHnIMzM8tR8QHRt2sz6jZvcgzAzK1HzAeHrMJmZ5av5r+7sNrSeqz70Bg7ae0SlSzEzG1RqPiAah9TxrtfvXekyzMwGnZofYjIzs3wOCDMzy+WAMDOzXA4IMzPL5YAwM7NcDggzM8vlgDAzs1wOCDMzy+WAMDOzXIUGhKSpkh6VtFzSBX1sc7KkJZIWS/pxpv00ScvSx2lF1mlmZi9X2KU2JNUBVwLHAa3AfElzImJJZpvJwIXAkRHxnKQ90/bRwFeBZiCABem+zxVVr5mZ9VZkD2IKsDwiVkTERmAWMK1km48DV/Z88EfEM2n7/wDui4hn03X3AVMLrNXMzEoUGRDjgVWZ5da0LWt/YH9Jv5X0oKSp/dgXSWdIapHU0tbWNoClm5lZpSep64HJwFHADOAaSaPK3Tkiro6I5ohoHjduXEElmpnVpiID4klgQma5KW3LagXmRERXRPwN+CtJYJSzr5mZFajIgJgPTJY0SVIDMB2YU7LNHSS9BySNJRlyWgHMBd4paQ9JewDvTNvMzGwHKexbTBHRLekskg/2OuD6iFgs6RKgJSLmsDUIlgCbgC9ExDoASV8jCRmASyLi2aJqNTOzl1NEVLqGAdHc3BwtLS2VLsPMbKciaUFENOetq/QktZmZDVIOCDMzy+WAMDOzXA4IMzPL5YAwM7NcDggzM8vlgDAzs1wOCDMzy+WAMDOzXA4IMzPL5YAwM7Nc2wwISe+V5CAxM6sx5XzwnwIsk/QNSQcWXZCZmQ0O2wyIiPgwcATwGHCDpN+nt/rcvfDqzMysYsoaOoqIDmA2MAvYG3g/8JCkzxRYm5mZVVA5cxAnSPop8AAwBJgSEe8CDgM+X2x5ZmZWKeXcUe5E4NsR8atsY0Ssl/SxYsoyM7NKKycgLgJW9yxIGgbsFRErI2JeUYWZmVlllTMH8RNgc2Z5U9pmZmZVrJyAqI+IjT0L6fOG4koyM7PBoJyAaJN0Qs+CpGnA2uJKMjOzwaCcOYhPAj+S9D1AwCrg1EKrMjOzittmQETEY8CbJA1Pl/9eeFVmZlZx5fQgkHQ8cAjQKAmAiLikwLrMzKzCyvmh3P8luR7TZ0iGmD4A7FtwXWZmVmHlTFK/JSJOBZ6LiIuBNwP7F1uWmZlVWjkB0Zn+u17SPkAXyfWYzMysipUTEHdKGgX8J/AQsBL4cTkvLmmqpEclLZd0Qc76mZLaJC1MH6dn1n1D0mJJSyVdoZ7JDzMz2yFecZI6vVHQvIh4HrhN0l1AY0S0b+uFJdUBVwLHAa3AfElzImJJyaa3RMRZJfu+BTgSODRt+g3wdpILBpqZ2Q7wij2IiNhM8iHfs/xSOeGQmgIsj4gV6a+vZwHTytw3gEaSX2wPJbmK7Joy9zUzswFQzhDTPEkn/gNDPONJflTXozVtK3WipEWSZkuaABARvwfuJ7lI4GpgbkQs7ef7m5nZdignID5BcnG+lyR1SHpBUscAvf+dwMSIOBS4D7gRQNJ+wEFAE0moHCPpraU7p3e2a5HU0tbWNkAlmZkZlHfL0d0jYpeIaIiIEenyiDJe+0lgQma5KW3Lvva6iHgpXbwWeGP6/P3AgxHx9/SX2/eQfL22tLarI6I5IprHjRtXRklmZlaucn4o97a8RxmvPR+YLGmSpAZgOjCn5LWzX5c9AegZRnoCeLukeklDSCaoPcRkZrYDlXOpjS9knjeSTD4vAI55pZ0iolvSWcBcoA64PiIWS7oEaImIOcDZ6ZViu4FngZnp7rPT1/8zyYT1vRFxZ9lHZWZm200R0b8dkonkyyPixGJK+sc0NzdHS0tLpcswM9upSFoQEc1568qZpC7VSjKBbGZmVWybQ0ySvksyzANJoBxO8otqMzOrYuXMQWTHbbqBmyPitwXVY2Zmg0Q5ATEb6IyITZBcQkPSrhGxvtjSzMysksr6JTUwLLM8DPh5MeWYmdlgUU5ANGZvM5o+37W4kszMbDAoJyBelPSGngVJbwQ2FFeSmZkNBuXMQXwW+Imkp0huOfoqkluQmplZFdtmQETEfEkHAgekTY9GRFexZZmZWaWVcy2mTwO7RcQjEfEIMFzSp4ovzczMKqmcOYiPp3eUAyAingM+XlxJZmY2GJQTEHXZmwWltxJtKK4kMzMbDMqZpL4XuEXS/0uXP0FyfwYzM6ti5QTEvwFnAJ9MlxeRfJPJzMyqWDl3lNsM/AFYSXIviGPwzXvMzKpenz0ISfsDM9LHWuAWgIg4eseUZmZmlfRKQ0x/AX4NvCcilgNI+twOqcrMzCrulYaY/hVYDdwv6RpJx5L8ktrMzGpAnwEREXdExHTgQOB+kktu7CnpKknv3FEFmplZZZQzSf1iRPw4It4LNAEPk3yzyczMqli/7kkdEc9FxNURcWxRBZmZ2eDQr4AwM7Pa4YAwM7NcDggzM8vlgDAzs1wOCDMzy+WAMDOzXA4IMzPLVWhASJoq6VFJyyVdkLN+pqQ2SQvTx+mZda+W9DNJSyUtkTSxyFrNzKy3cu4H8Q9J7zx3JXAc0ArMlzQnIpaUbHpLRJyV8xI3AZdGxH2ShgObi6rVzMxersgexBRgeUSsiIiNwCxgWjk7SjoYqI+I+wAi4u8Rsb64Us3MrFSRATEeWJVZbk3bSp0oaZGk2ZImpG37A89Lul3Sw5L+M+2R9CLpDEktklra2toG/gjMzGpYpSep7wQmRsShwH3AjWl7PfBW4Dzgn4DXADNLd06vC9UcEc3jxo3bMRWbmdWIIgPiSWBCZrkpbdsiItZFxEvp4rXAG9PnrcDCdHiqG7gDeEOBtZqZWYkiA2I+MFnSJEkNwHRgTnYDSXtnFk9g672u5wOjJPV0C44BSie3zcysQIV9iykiuiWdBcwF6oDrI2KxpEuAloiYA5wt6QSgG3iWdBgpIjZJOg+YJ0nAAuCaomo1M7OXU0RUuoYB0dzcHC0tLZUuw8xspyJpQUQ0562r9CS1mZkNUg4IMzPL5YAwM7NcDggzM8vlgDAzs1wOCDMzy+WAMDOzXA4IMzPL5YAwM7NcDggzM8vlgDAzs1wOCDMzy+WAMDOzXA4IMzPL5YAwM7NcDggzM8vlgDAzs1wOCDMzy+WAMDOzXA4IMzPL5YAwM7NcDggzM8vlgDAzs1wOCDMzy+WAMDOzXA4IMzPL5YAwM7NcDggzM8tVaEBImirpUUnLJV2Qs36mpDZJC9PH6SXrR0hqlfS9Ius0M7OXqy/qhSXVAVcCxwGtwHxJcyJiScmmt0TEWX28zNeAXxVVo5nVtq6uLlpbW+ns7Kx0KYVrbGykqamJIUOGlL1PYQEBTAGWR8QKAEmzgGlAaUDkkvRGYC/gXqC5qCLNrHa1tray++67M3HiRCRVupzCRATr1q2jtbWVSZMmlb1fkUNM44FVmeXWtK3UiZIWSZotaQKApF2AbwLnvdIbSDpDUouklra2toGq28xqRGdnJ2PGjKnqcACQxJgxY/rdU6r0JPWdwMSIOBS4D7gxbf8UcHdEtL7SzhFxdUQ0R0TzuHHjCi7VzKpRtYdDj3/kOIscYnoSmJBZbkrbtoiIdZnFa4FvpM/fDLxV0qeA4UCDpL9HxMsmus3MrBhFBsR8YLKkSSTBMB34YHYDSXtHxOp08QRgKUBEfCizzUyg2eFgZtVm3bp1HHvssQA8/fTT1NXV0TMa8sc//pGGhoY+921paeGmm27iiiuuKKy+wgIiIrolnQXMBeqA6yNisaRLgJaImAOcLekEoBt4FphZVD1mZoPNmDFjWLhwIQAXXXQRw4cP57zztk69dnd3U1+f/zHd3NxMc3Ox398psgdBRNwN3F3S9pXM8wuBC7fxGjcANxRQnpnZFhffuZglT3UM6GsevM8IvvreQ/q1z8yZM2lsbOThhx/myCOPZPr06Zxzzjl0dnYybNgwvv/973PAAQfwwAMPcNlll3HXXXdx0UUX8cQTT7BixQqeeOIJPvvZz3L22Wdvd/2FBoSZmfVfa2srv/vd76irq6Ojo4Nf//rX1NfX8/Of/5wvfvGL3HbbbS/b5y9/+Qv3338/L7zwAgcccABnnnlmv37zkMcBYWYG/f5Lv0gf+MAHqKurA6C9vZ3TTjuNZcuWIYmurq7cfY4//niGDh3K0KFD2XPPPVmzZg1NTU3bVUelv+ZqZmYldtttty3Pv/zlL3P00UfzyCOPcOedd/b5W4ahQ4dueV5XV0d3d/d21+GAMDMbxNrb2xk/PvmN8Q033LBD39sBYWY2iJ1//vlceOGFHHHEEQPSK+gPRcQOfcOiNDc3R0tLS6XLMLOdyNKlSznooIMqXcYOk3e8khZERO73Zd2DMDOzXA4IMzPL5YAwM7NcDggzM8vlgDAzs1wOCDMzy+WAMDOrkKOPPpq5c+f2arv88ss588wzc7c/6qij2JFf53dAmJlVyIwZM5g1a1avtlmzZjFjxowKVdSbL9ZnZgZwzwXw9J8H9jVf9Xp413/0ufqkk07iS1/6Ehs3bqShoYGVK1fy1FNPcfPNN3PuueeyYcMGTjrpJC6++OKBratM7kGYmVXI6NGjmTJlCvfccw+Q9B5OPvlkLr30UlpaWli0aBG//OUvWbRoUUXqcw/CzAxe8S/9IvUMM02bNo1Zs2Zx3XXXceutt3L11VfT3d3N6tWrWbJkCYceeugOr809CDOzCpo2bRrz5s3joYceYv369YwePZrLLruMefPmsWjRIo4//vg+L/FdNAeEmVkFDR8+nKOPPpqPfvSjzJgxg46ODnbbbTdGjhzJmjVrtgw/VYKHmKCYySkzG/xedz6srfzH4Izj3877b72VWVf9BweO35UjDnotB05+LRPGv4oj/+kweGENrF0GXRvg+Sdg7cjeLzBkGIzcvrvH5an8fxkzsxr3vncfR7T9dcvyDd/7eu52D/zXD3dUSYADIlGhySkzq7ClS2Hs5EpXMWh5DsLMzHI5IMysplXLXTW35R85TgeEmdWsxsZG1q1bV/UhERGsW7eOxsbGfu3nOQgzq1lNTU20trbS1tZW6VIK19jYSFNT/77p5IAws5o1ZMgQJk2aVOkyBi0PMZmZWS4HhJmZ5XJAmJlZLlXL7L2kNuDx7XiJscDaASpnZ1GLxwy1edy1eMxQm8fd32PeNyLG5a2omoDYXpJaIqK50nXsSLV4zFCbx12Lxwy1edwDecweYjIzs1wOCDMzy+WA2OrqShdQAbV4zFCbx12Lxwy1edwDdsyegzAzs1zuQZiZWS4HhJmZ5ar5gJA0VdKjkpZLuqDS9RRF0gRJ90taImmxpHPS9tGS7pO0LP13j0rXOtAk1Ul6WNJd6fIkSX9Iz/ktkhoqXeNAkzRK0mxJf5G0VNKbq/1cS/pc+v/2I5JultRYjeda0vWSnpH0SKYt99wqcUV6/IskvaE/71XTASGpDrgSeBdwMDBD0sGVraow3cDnI+Jg4E3Ap9NjvQCYFxGTgXnpcrU5B1iaWf468O2I2A94DvhYRaoq1neAeyPiQOAwkuOv2nMtaTxwNtAcEa8D6oDpVOe5vgGYWtLW17l9FzA5fZwBXNWfN6rpgACmAMsjYkVEbARmAdMqXFMhImJ1RDyUPn+B5ANjPMnx3phudiPwvspUWAxJTcDxwLXpsoBjgNnpJtV4zCOBtwHXAUTExoh4nio/1yRXpx4mqR7YFVhNFZ7riPgV8GxJc1/ndhpwUyQeBEZJ2rvc96r1gBgPrMost6ZtVU3SROAI4A/AXhGxOl31NLBXhcoqyuXA+cDmdHkM8HxEdKfL1XjOJwFtwPfTobVrJe1GFZ/riHgSuAx4giQY2oEFVP+57tHXud2uz7haD4iaI2k4cBvw2YjoyK6L5DvPVfO9Z0nvAZ6JiAWVrmUHqwfeAFwVEUcAL1IynFSF53oPkr+WJwH7ALvx8mGYmjCQ57bWA+JJYEJmuSltq0qShpCEw48i4va0eU1PlzP995lK1VeAI4ETJK0kGT48hmRsflQ6DAHVec5bgdaI+EO6PJskMKr5XL8D+FtEtEVEF3A7yfmv9nPdo69zu12fcbUeEPOByek3HRpIJrXmVLimQqRj79cBSyPiW5lVc4DT0uenAf+1o2srSkRcGBFNETGR5Nz+IiI+BNwPnJRuVlXHDBARTwOrJB2QNh0LLKGKzzXJ0NKbJO2a/r/ec8xVfa4z+jq3c4BT028zvQlozwxFbVPN/5Ja0rtJxqnrgOsj4tIKl1QISf8C/Br4M1vH479IMg9xK/BqksulnxwRpRNgOz1JRwHnRcR7JL2GpEcxGngY+HBEvFTJ+gaapMNJJuYbgBXAR0j+IKzacy3pYuAUkm/sPQycTjLeXlXnWtLNwFEkl/VeA3wVuIOcc5uG5fdIhtvWAx+JiJay36vWA8LMzPLV+hCTmZn1wQFhZma5HBBmZpbLAWFmZrkcEGZmlssBYdYPkjZJWph5DNgF7yRNzF6h06zS6re9iZllbIiIwytdhNmO4B6E2QCQtFLSNyT9WdIfJe2Xtk+U9Iv0WvzzJL06bd9L0k8l/Sl9vCV9qTpJ16T3NfiZpGEVOyireQ4Is/4ZVjLEdEpmXXtEvJ7kl6uXp23fBW6MiEOBHwFXpO1XAL+MiMNIrpO0OG2fDFwZEYcAzwMnFnw8Zn3yL6nN+kHS3yNieE77SuCYiFiRXhTx6YgYI2ktsHdEdKXtqyNirKQ2oCl72Yf0Muz3pTd9QdK/AUMi4t+LPzKzl3MPwmzgRB/P+yN7naBNeJ7QKsgBYTZwTsn8+/v0+e9IriQL8CGSCyZCclvIM2HLPbNH7qgizcrlv07M+meYpIWZ5XsjouerrntIWkTSC5iRtn2G5M5uXyC5y9tH0vZzgKslfYykp3AmyZ3QzAYNz0GYDYB0DqI5ItZWuhazgeIhJjMzy+UehJmZ5XIPwszMcjkgzMwslwPCzMxyOSDMzCyXA8LMzHL9N1rrvGXalZEBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "MF35F4OQYDgP",
        "outputId": "4b4ee789-d85f-4823-847b-37a99854631a"
      },
      "source": [
        "\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwI-rcowv0op"
      },
      "source": [
        "import tensorflow\n",
        "import keras\n",
        "\n",
        "#from numpy import loadtxt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import backend\n",
        "\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGWkCBeSvZlT"
      },
      "source": [
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=26, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(2, activation='linear'))\n",
        "\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlOLM4Siw-e2",
        "outputId": "c364065c-88b2-4c2a-83d8-a6f48ffc87f2"
      },
      "source": [
        "model.compile(loss='mean_squared_error', optimizer='adam',metrics=['mse'])\n",
        "model.summary()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_66 (Dense)             (None, 32)                864       \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 1,986\n",
            "Trainable params: 1,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njYybf8xxLu7",
        "outputId": "7feb6d05-95b2-48a1-8e24-ad1abb05f2d3"
      },
      "source": [
        "model.fit(X, Y, epochs=100, batch_size=32)\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan  \n",
            "Epoch 2/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - 0s 2ms/step - loss: nan - mse: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4866170ed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-B0n6qILlvP",
        "outputId": "a49156c6-8656-44c5-ea8d-beb34d7db121"
      },
      "source": [
        "# evaluate the keras model\n",
        "loss,mse = model.evaluate(X, Y)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 0s 1ms/step - loss: nan - mse: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_K3ZE2WQL2FP",
        "outputId": "b8558ee2-d05d-4d63-812f-d34658ae3a4b"
      },
      "source": [
        "print('Loss: %.2f' % loss)\n",
        "print('mse: %.2f' % mse)\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: nan\n",
            "mse: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3nCp96jNKeH",
        "outputId": "1417b6d3-cf43-46f1-d9a4-bec503acf376"
      },
      "source": [
        "predic=model.predict(X)\n",
        "print(X)\n",
        "print(\"X=%s, Predicted=%s\" % (X[0], Y[0]))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.67046481  0.82543008  0.30480033 ... -0.38866169  0.\n",
            "  -0.14112307]\n",
            " [-0.67046481 -0.8953464  -0.43173627 ... -0.38866169  0.\n",
            "  -0.17228042]\n",
            " [-0.67046481 -0.8953464  -0.34187176 ... -0.38866169  0.\n",
            "  -0.17748711]\n",
            " ...\n",
            " [ 1.45599165 -0.08556924 -0.38944709 ... -0.38866169  0.\n",
            "  -0.17796327]\n",
            " [ 1.45599165  0.31931935 -0.39649529 ... -0.38866169  0.\n",
            "  -0.17804608]\n",
            " [ 1.45599165  0.01565291 -0.43173627 ... -0.38866169  0.\n",
            "  -0.17792186]]\n",
            "X=[-0.67046481  0.82543008  0.30480033  1.42984618  0.33650215 -0.16582948\n",
            " -0.6667522  -0.194121   -0.12750609 -0.25599783 -0.29528463 -0.28481157\n",
            " -0.16161448 -0.56989549 -0.18999669 -0.41140172 -0.1650196  -0.41041721\n",
            " -0.19669061 -0.24656792 -0.17539763 -0.35250258 -0.16646776 -0.38866169\n",
            "  0.         -0.14112307], Predicted=1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXpgWt14Spmb",
        "outputId": "09378003-2e5e-4bbd-94d0-778cb8012053"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy\n",
        "# define 10-fold cross validation \n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=8)\n",
        "cvscores = []\n",
        "for train, test in kfold.split(X, Y):\n",
        "\n",
        "  # create model\n",
        "  model = Sequential()\n",
        "  model.add(Dense(32, input_dim=26, activation='relu'))\n",
        "  model.add(Dense(32, activation='relu'))\n",
        "  model.add(Dense(2, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# Fit the model\n",
        "  model.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
        "\t# evaluate the model\n",
        "  scores = model.evaluate(X[test], Y[test], verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  cvscores.append(scores[1] * 100)\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 60.26%\n",
            "accuracy: 60.26%\n",
            "accuracy: 97.44%\n",
            "WARNING:tensorflow:5 out of the last 35 calls to <function Model.make_test_function.<locals>.test_function at 0x7f4861bd28c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy: 60.26%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f485f9590e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy: 60.26%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f485d6b2b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy: 61.54%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f485820c440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy: 61.04%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f48570f9d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy: 61.04%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f4855f62680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy: 61.04%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f487368d3b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy: 61.04%\n",
            "64.42% (+/- 11.02%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}